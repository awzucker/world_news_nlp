{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World News NLP Project\n",
    "## Scratchpad\n",
    "#### Adam Zucker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "- __*world_news_posts.csv*:__ Supplied dataframe with roughly 500,000 titles of posts on a \"world news\" message board, including data for the date, time, and author of the post, along with user interaction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "df = pd.read_csv('../data/world_news_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls in the dataframe - none detected\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data spans 3223 days, from 1/25/08 to 11/22/16\n",
    "print(f\"Number of days represented in dataframe: {len(df['date_created'].unique())}\")\n",
    "print(f\"Data date range is from {min(df['date_created'])} to {max(df['date_created'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to concisely process this dataframe and others in the same format\n",
    "def process_data(df):\n",
    "    \n",
    "    # Redefining the 'time_created' column to hold datetime, converted from unix timestamp format\n",
    "    df['time_created'] = [datetime.fromtimestamp(ts) for ts in df['time_created']]\n",
    "    # Dropping 'date_created' because of redundancy\n",
    "    df.drop(columns='date_created', inplace=True)\n",
    "    \n",
    "    # Creating a feature to hold the post length in characters and words\n",
    "    df['post_length_chars'] = df['title'].apply(len)\n",
    "    df['post_length_tokens'] = df['title'].str.split().apply(len)\n",
    "    \n",
    "#     # Generating features to hold total author posts and total author upvotes alongside each post\n",
    "#     df['author_posts'] = df['author'].groupby(df['author']).transform('count')\n",
    "#     df['author_upvotes'] = [df['up_votes'].groupby(df['author']).sum() for a in df['author']]\n",
    "    \n",
    "    # Generating a feature to hold day of the week and dummifying\n",
    "    df['weekday'] = df['time_created'].dt.day_name()\n",
    "    day_dummies = pd.get_dummies(df['weekday'], drop_first=True)\n",
    "    df = pd.concat([df, day_dummies], axis=1)\n",
    "    df.drop(columns='weekday', inplace=True)\n",
    "    \n",
    "    # Dropping 'category' feature if only one category is present\n",
    "    if len(df['category'].unique()) == 1:\n",
    "        df.drop(columns='category', inplace=True)\n",
    "    # Similarly dropping down votes if there are none reported\n",
    "    if sum(df['down_votes']) == 0:\n",
    "        df.drop(columns='down_votes', inplace=True)\n",
    "    \n",
    "    # Binarizing 'over_18' feature\n",
    "    df['over_18'] = df['over_18'].map({False:0, True:1})\n",
    "    \n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['up_votes'].groupby(df['author']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting 'date_created' to datetime\n",
    "# df['date_created'] = pd.to_datetime(df['date_created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All posts are classified as 'worldnews' - with just a single class represented, this feature becomes unnecessary\n",
    "# df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dropping 'category' feature\n",
    "# df.drop(columns='category', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats for upvotes\n",
    "df['up_votes'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at titles of most upvoted posts\n",
    "df['up_votes'].groupby(df['title']).sum().sort_values(ascending=False)[0:10].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('up_votes', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique authors: {len(df['author'].unique())}\")\n",
    "print('-----')\n",
    "print(f\"Top 20 contributors by post count: \\n{df['author'].value_counts()[0:20]}\")\n",
    "print('-----')\n",
    "print(f\"Top 20 contributors by upvotes: \\n{df['up_votes'].groupby(df['author']).sum().sort_values(ascending=False)[0:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at distribution of 'over_18' posts by number and percentage\n",
    "print(df['over_18'].value_counts())\n",
    "print(df['over_18'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking title content of some of the posts classified as \"over_18\"\n",
    "df[df['over_18'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsfw = df[df['over_18'] == True]\n",
    "nsfw.sort_values(by='up_votes', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = df['title'][111111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'][111111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://spacy.io/ demo code\n",
    "doc = nlp(test_title)\n",
    "\n",
    "print([noun_phrases.text for noun_phrases in doc.noun_chunks])\n",
    "print('-----')\n",
    "print([token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "print('-----')\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((range(len(df.index)))[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating columns of empty lists to hold NLP output\n",
    "\n",
    "# df['noun_phrases'] = df.apply(lambda value: [], axis=1)\n",
    "# df['verbs'] = df.apply(lambda value: [], axis=1)\n",
    "# df['entities'] = df.apply(lambda value: [], axis=1)\n",
    "# df['entity_labels'] = df.apply(lambda value: [], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The `lambda` function above is necessary since I can't directly assign an empty list as a value to fill the new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiating spacy NLP\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Defining a new function to segment post titles into component pieces and insert into original dataframe\n",
    "# def title_deconstruct(df):\n",
    "#     for i in range(len(df.index)):\n",
    "#         title = df['title'][i]\n",
    "#         doc = nlp(title)\n",
    "#         df['noun_chunks'][i] = [noun_chunk.text for noun_chunk in doc.noun_chunks]\n",
    "#         df['verbs'][i] = [verb.lemma_ for verb in doc if verb.pos_ == \"VERB\"]\n",
    "#         df['entities'][i] = [entity.text for entity in doc.ents]\n",
    "#         df['entity_labels'][i] = [entity.label_ for entity in doc.ents]\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_deconstruct(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initializing a new, empty dataframe to hold nlp data\n",
    "# nlp_df = pd.DataFrame(data=None, index=range(len(df.index)), columns=['noun_chunks', 'verbs', 'entities', 'entity_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiating spacy NLP\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Defining a new function to segment post titles into component pieces and insert into original dataframe\n",
    "# def title_deconstruct(df):\n",
    "#     for i in range(len(df)):\n",
    "#         title = df['title'][i]\n",
    "#         doc = nlp(title)\n",
    "#         nlp_df['noun_chunks'][i] = [noun_chunk.text for noun_chunk in doc.noun_chunks]\n",
    "#         nlp_df['verbs'][i] = [verb.lemma_ for verb in doc if verb.pos_ == \"VERB\"]\n",
    "#         nlp_df['entities'][i] = [entity.text for entity in doc.ents]\n",
    "#         nlp_df['entity_labels'][i] = [entity.label_ for entity in doc.ents]\n",
    "#     return nlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_df = title_deconstruct(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([df, nlp_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiating spacy NLP\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Defining a new function to segment post titles into component pieces and insert into original dataframe\n",
    "# def title_deconstruct(df):\n",
    "#     for i in range(10):\n",
    "#         title = df['title'][i]\n",
    "#         doc = nlp(title)\n",
    "#         nouns = [noun_chunk.text for noun_chunk in doc.noun_chunks]\n",
    "#         verbs = [verb.lemma_ for verb in doc if verb.pos_ == \"VERB\"]\n",
    "#         entities = [entity.text for entity in doc.ents]\n",
    "#         ent_labels = [entity.label_ for entity in doc.ents]\n",
    "#         df['noun_chunks'][i].append(nouns) \n",
    "#         df['verbs'][i].append(verbs) \n",
    "#         df['entities'][i].append(entities)\n",
    "#         df['entity_labels'][i].append(ent_labels)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_deconstruct(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### This one works!\n",
    "\n",
    "**BELOW:** This seems to be the best iteration of the function, but is still computationally inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating spacy NLP\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Defining a new function to segment post titles into component pieces and insert into original dataframe\n",
    "def title_deconstruct(df):\n",
    "    for i in range(len(df)):\n",
    "        title = df['title'][i]\n",
    "        doc = nlp(title)\n",
    "        df.at[i, 'noun_phrases'] = [noun_chunk.text for noun_chunk in doc.noun_chunks]\n",
    "        df.at[i, 'verbs'] = [verb.lemma_ for verb in doc if verb.pos_ == \"VERB\"]\n",
    "        df.at[i, 'entities'] = [entity.text for entity in doc.ents]\n",
    "        df.at[i, 'entity_labels'] = [entity.label_ for entity in doc.ents]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = title_deconstruct(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST\n",
    "\n",
    "Combining it all into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "df = pd.read_csv('../data/world_news_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>date_created</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>title</th>\n",
       "      <th>over_18</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1201232046</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1201232075</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1201232523</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_created date_created  up_votes  down_votes  \\\n",
       "0    1201232046   2008-01-25         3           0   \n",
       "1    1201232075   2008-01-25         2           0   \n",
       "2    1201232523   2008-01-25         3           0   \n",
       "\n",
       "                               title  over_18 author   category  \n",
       "0  Scores killed in Pakistan clashes    False  polar  worldnews  \n",
       "1   Japan resumes refuelling mission    False  polar  worldnews  \n",
       "2    US presses Egypt on Gaza border    False  polar  worldnews  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 509236 entries, 0 to 509235\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   time_created  509236 non-null  int64 \n",
      " 1   date_created  509236 non-null  object\n",
      " 2   up_votes      509236 non-null  int64 \n",
      " 3   down_votes    509236 non-null  int64 \n",
      " 4   title         509236 non-null  object\n",
      " 5   over_18       509236 non-null  bool  \n",
      " 6   author        509236 non-null  object\n",
      " 7   category      509236 non-null  object\n",
      "dtypes: bool(1), int64(3), object(4)\n",
      "memory usage: 27.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BELOW:** The `process_data` function defined here will generate and populate the existing dataframe with a number of new features, as well as drop unnecessary features.\n",
    "\n",
    "TO DO\n",
    "* Fix the upvotes by author feature\n",
    "* Thoroughly verify data integrity\n",
    "* Make sure the code is clear and flexible, eg, add loops to account for nulls in source data\n",
    "* Is there a way to make it more efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating columns of empty lists to hold NLP output\n",
    "\n",
    "df['noun_phrases'] = df.apply(lambda value: [], axis=1)\n",
    "df['verbs'] = df.apply(lambda value: [], axis=1)\n",
    "df['entities'] = df.apply(lambda value: [], axis=1)\n",
    "df['entity_labels'] = df.apply(lambda value: [], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Defining a function to concisely process this dataframe and others in the same format\n",
    "def process_data(df):\n",
    "    \n",
    "    # Redefining the 'time_created' column to hold datetime, converted from unix timestamp format\n",
    "    df['time_created'] = [datetime.fromtimestamp(ts) for ts in df['time_created']]\n",
    "    # Dropping 'date_created' because of redundancy\n",
    "    df.drop(columns='date_created', inplace=True)\n",
    "    \n",
    "    # Dropping 'category' feature if only one category is present\n",
    "    if len(df['category'].unique()) == 1:\n",
    "        df.drop(columns='category', inplace=True)\n",
    "    # Similarly dropping down votes if there are none reported\n",
    "    if sum(df['down_votes']) == 0:\n",
    "        df.drop(columns='down_votes', inplace=True)\n",
    "    \n",
    "    # Binarizing 'over_18' feature\n",
    "    df['over_18'] = df['over_18'].map({False:0, True:1})\n",
    "    \n",
    "    # Creating a feature to hold the post length in characters and words\n",
    "    df['post_length_chars'] = df['title'].apply(len)\n",
    "    df['post_length_tokens'] = df['title'].str.split().apply(len)\n",
    "    \n",
    "    # Generating features to hold total author posts and total author upvotes alongside each post\n",
    "    df['author_posts'] = df['author'].groupby(df['author']).transform('count')\n",
    "#     df['author_upvotes'] = [df['up_votes'].groupby(df['author']).sum() for a in df['author']]\n",
    "    \n",
    "    # Generating a feature to hold day of the week and dummifying\n",
    "    df['weekday'] = df['time_created'].dt.day_name()\n",
    "    day_dummies = pd.get_dummies(df['weekday'], drop_first=True)\n",
    "    df = pd.concat([df, day_dummies], axis=1)\n",
    "    df.drop(columns='weekday', inplace=True)\n",
    "    \n",
    "    # Try insertin NLP feature generation here, instead of outside the function **********\n",
    "    \n",
    "    # Instantiating spacy NLP\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Incorporating the loop from 'title_deconstruct' function to segment post titles into component pieces and insert into original dataframe\n",
    "    for i in range(len(df)):\n",
    "        title = df['title'][i]\n",
    "        doc = nlp(title)\n",
    "        df.at[i, 'noun_phrases'] = [noun_chunk.text for noun_chunk in doc.noun_chunks]\n",
    "        df.at[i, 'verbs'] = [verb.lemma_ for verb in doc if verb.pos_ == \"VERB\"]\n",
    "        df.at[i, 'entities'] = [entity.text for entity in doc.ents]\n",
    "        df.at[i, 'entity_labels'] = [entity.label_ for entity in doc.ents]\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>title</th>\n",
       "      <th>over_18</th>\n",
       "      <th>author</th>\n",
       "      <th>noun_phrases</th>\n",
       "      <th>verbs</th>\n",
       "      <th>entities</th>\n",
       "      <th>entity_labels</th>\n",
       "      <th>post_length_chars</th>\n",
       "      <th>post_length_tokens</th>\n",
       "      <th>author_posts</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-01-24 22:34:06</td>\n",
       "      <td>3</td>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "      <td>0</td>\n",
       "      <td>polar</td>\n",
       "      <td>[Scores, Pakistan clashes]</td>\n",
       "      <td>[kill]</td>\n",
       "      <td>[Pakistan]</td>\n",
       "      <td>[GPE]</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-01-24 22:34:35</td>\n",
       "      <td>2</td>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "      <td>0</td>\n",
       "      <td>polar</td>\n",
       "      <td>[Japan, refuelling mission]</td>\n",
       "      <td>[resume]</td>\n",
       "      <td>[Japan]</td>\n",
       "      <td>[GPE]</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-01-24 22:42:03</td>\n",
       "      <td>3</td>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "      <td>0</td>\n",
       "      <td>polar</td>\n",
       "      <td>[US, Egypt, Gaza border]</td>\n",
       "      <td>[press]</td>\n",
       "      <td>[US, Egypt, Gaza]</td>\n",
       "      <td>[GPE, GPE, GPE]</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-01-24 22:54:50</td>\n",
       "      <td>1</td>\n",
       "      <td>Jump-start economy: Give health care to all</td>\n",
       "      <td>0</td>\n",
       "      <td>fadi420</td>\n",
       "      <td>[Jump-start economy, health care]</td>\n",
       "      <td>[give]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-01-25 10:25:20</td>\n",
       "      <td>4</td>\n",
       "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
       "      <td>0</td>\n",
       "      <td>mhermans</td>\n",
       "      <td>[Council, Europe, EU&amp;UN]</td>\n",
       "      <td>[bash]</td>\n",
       "      <td>[Council of Europe, EU&amp;UN]</td>\n",
       "      <td>[ORG, ORG]</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509231</th>\n",
       "      <td>2016-11-22 07:12:44</td>\n",
       "      <td>5</td>\n",
       "      <td>Heil Trump : Donald Trump s  alt-right  white...</td>\n",
       "      <td>0</td>\n",
       "      <td>nonamenoglory</td>\n",
       "      <td>[ Heil Trump, Donald Trump, alt-right  white n...</td>\n",
       "      <td>[s, invoke]</td>\n",
       "      <td>[Heil Trump, Donald Trump, Nazi]</td>\n",
       "      <td>[PERSON, PERSON, NORP]</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509232</th>\n",
       "      <td>2016-11-22 07:12:52</td>\n",
       "      <td>1</td>\n",
       "      <td>There are people speculating that this could b...</td>\n",
       "      <td>0</td>\n",
       "      <td>SummerRay</td>\n",
       "      <td>[people, Madeleine McCann]</td>\n",
       "      <td>[speculate, be]</td>\n",
       "      <td>[Madeleine McCann]</td>\n",
       "      <td>[PERSON]</td>\n",
       "      <td>67</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509233</th>\n",
       "      <td>2016-11-22 07:17:36</td>\n",
       "      <td>1</td>\n",
       "      <td>Professor receives Arab Researchers Award</td>\n",
       "      <td>0</td>\n",
       "      <td>AUSharjah</td>\n",
       "      <td>[Professor, Arab Researchers Award]</td>\n",
       "      <td>[receive]</td>\n",
       "      <td>[Arab]</td>\n",
       "      <td>[NORP]</td>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509234</th>\n",
       "      <td>2016-11-22 07:19:17</td>\n",
       "      <td>1</td>\n",
       "      <td>Nigel Farage attacks response to Trump ambassa...</td>\n",
       "      <td>0</td>\n",
       "      <td>smilyflower</td>\n",
       "      <td>[Nigel Farage, response, Trump ambassador tweet]</td>\n",
       "      <td>[attack]</td>\n",
       "      <td>[Nigel Farage, Trump]</td>\n",
       "      <td>[PERSON, ORG]</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509235</th>\n",
       "      <td>2016-11-22 07:22:26</td>\n",
       "      <td>1</td>\n",
       "      <td>Palestinian wielding knife shot dead in West B...</td>\n",
       "      <td>0</td>\n",
       "      <td>superislam</td>\n",
       "      <td>[Palestinian wielding knife, West Bank, Israel...</td>\n",
       "      <td>[shoot]</td>\n",
       "      <td>[Palestinian, West Bank, Israel]</td>\n",
       "      <td>[NORP, GPE, GPE]</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509236 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              time_created  up_votes  \\\n",
       "0      2008-01-24 22:34:06         3   \n",
       "1      2008-01-24 22:34:35         2   \n",
       "2      2008-01-24 22:42:03         3   \n",
       "3      2008-01-24 22:54:50         1   \n",
       "4      2008-01-25 10:25:20         4   \n",
       "...                    ...       ...   \n",
       "509231 2016-11-22 07:12:44         5   \n",
       "509232 2016-11-22 07:12:52         1   \n",
       "509233 2016-11-22 07:17:36         1   \n",
       "509234 2016-11-22 07:19:17         1   \n",
       "509235 2016-11-22 07:22:26         1   \n",
       "\n",
       "                                                    title  over_18  \\\n",
       "0                       Scores killed in Pakistan clashes        0   \n",
       "1                        Japan resumes refuelling mission        0   \n",
       "2                         US presses Egypt on Gaza border        0   \n",
       "3            Jump-start economy: Give health care to all         0   \n",
       "4         Council of Europe bashes EU&UN terror blacklist        0   \n",
       "...                                                   ...      ...   \n",
       "509231   Heil Trump : Donald Trump s  alt-right  white...        0   \n",
       "509232  There are people speculating that this could b...        0   \n",
       "509233          Professor receives Arab Researchers Award        0   \n",
       "509234  Nigel Farage attacks response to Trump ambassa...        0   \n",
       "509235  Palestinian wielding knife shot dead in West B...        0   \n",
       "\n",
       "               author                                       noun_phrases  \\\n",
       "0               polar                         [Scores, Pakistan clashes]   \n",
       "1               polar                        [Japan, refuelling mission]   \n",
       "2               polar                           [US, Egypt, Gaza border]   \n",
       "3             fadi420                  [Jump-start economy, health care]   \n",
       "4            mhermans                           [Council, Europe, EU&UN]   \n",
       "...               ...                                                ...   \n",
       "509231  nonamenoglory  [ Heil Trump, Donald Trump, alt-right  white n...   \n",
       "509232      SummerRay                         [people, Madeleine McCann]   \n",
       "509233      AUSharjah                [Professor, Arab Researchers Award]   \n",
       "509234    smilyflower   [Nigel Farage, response, Trump ambassador tweet]   \n",
       "509235     superislam  [Palestinian wielding knife, West Bank, Israel...   \n",
       "\n",
       "                  verbs                          entities  \\\n",
       "0                [kill]                        [Pakistan]   \n",
       "1              [resume]                           [Japan]   \n",
       "2               [press]                 [US, Egypt, Gaza]   \n",
       "3                [give]                                []   \n",
       "4                [bash]        [Council of Europe, EU&UN]   \n",
       "...                 ...                               ...   \n",
       "509231      [s, invoke]  [Heil Trump, Donald Trump, Nazi]   \n",
       "509232  [speculate, be]                [Madeleine McCann]   \n",
       "509233        [receive]                            [Arab]   \n",
       "509234         [attack]             [Nigel Farage, Trump]   \n",
       "509235          [shoot]  [Palestinian, West Bank, Israel]   \n",
       "\n",
       "                 entity_labels  post_length_chars  post_length_tokens  \\\n",
       "0                        [GPE]                 33                   5   \n",
       "1                        [GPE]                 32                   4   \n",
       "2              [GPE, GPE, GPE]                 31                   6   \n",
       "3                           []                 44                   7   \n",
       "4                   [ORG, ORG]                 47                   7   \n",
       "...                        ...                ...                 ...   \n",
       "509231  [PERSON, PERSON, NORP]                 88                  13   \n",
       "509232                [PERSON]                 67                  10   \n",
       "509233                  [NORP]                 41                   5   \n",
       "509234           [PERSON, ORG]                 55                   8   \n",
       "509235        [NORP, GPE, GPE]                 64                  10   \n",
       "\n",
       "        author_posts  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \n",
       "0                 50       0         0       0         1        0          0  \n",
       "1                 50       0         0       0         1        0          0  \n",
       "2                 50       0         0       0         1        0          0  \n",
       "3                  2       0         0       0         1        0          0  \n",
       "4                  1       0         0       0         0        0          0  \n",
       "...              ...     ...       ...     ...       ...      ...        ...  \n",
       "509231             5       0         0       0         0        1          0  \n",
       "509232             1       0         0       0         0        1          0  \n",
       "509233             3       0         0       0         0        1          0  \n",
       "509234            52       0         0       0         0        1          0  \n",
       "509235           177       0         0       0         0        1          0  \n",
       "\n",
       "[509236 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_created          0\n",
       "up_votes              0\n",
       "title                 0\n",
       "over_18               0\n",
       "author                0\n",
       "noun_phrases          0\n",
       "verbs                 0\n",
       "entities              0\n",
       "entity_labels         0\n",
       "post_length_chars     0\n",
       "post_length_tokens    0\n",
       "author_posts          0\n",
       "Monday                0\n",
       "Saturday              0\n",
       "Sunday                0\n",
       "Thursday              0\n",
       "Tuesday               0\n",
       "Wednesday             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 509236 entries, 0 to 509235\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   time_created        509236 non-null  datetime64[ns]\n",
      " 1   up_votes            509236 non-null  int64         \n",
      " 2   title               509236 non-null  object        \n",
      " 3   over_18             509236 non-null  int64         \n",
      " 4   author              509236 non-null  object        \n",
      " 5   noun_phrases        509236 non-null  object        \n",
      " 6   verbs               509236 non-null  object        \n",
      " 7   entities            509236 non-null  object        \n",
      " 8   entity_labels       509236 non-null  object        \n",
      " 9   post_length_chars   509236 non-null  int64         \n",
      " 10  post_length_tokens  509236 non-null  int64         \n",
      " 11  author_posts        509236 non-null  int64         \n",
      " 12  Monday              509236 non-null  uint8         \n",
      " 13  Saturday            509236 non-null  uint8         \n",
      " 14  Sunday              509236 non-null  uint8         \n",
      " 15  Thursday            509236 non-null  uint8         \n",
      " 16  Tuesday             509236 non-null  uint8         \n",
      " 17  Wednesday           509236 non-null  uint8         \n",
      "dtypes: datetime64[ns](1), int64(5), object(6), uint8(6)\n",
      "memory usage: 49.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_created', 'up_votes', 'title', 'over_18', 'author',\n",
       "       'noun_phrases', 'verbs', 'entities', 'entity_labels',\n",
       "       'post_length_chars', 'post_length_tokens', 'author_posts', 'Monday',\n",
       "       'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns as last step in function?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
